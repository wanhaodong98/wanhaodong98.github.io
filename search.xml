<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hello World</title>
    <url>/2020/11/02/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.<br><a id="more"></a></p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>强化学习Sutton-Chapter3-有限马尔可夫决策过程</title>
    <url>/2020/11/13/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0Sutton-Chapter3-%E6%9C%89%E9%99%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/</url>
    <content><![CDATA[<h3 id="智能体环境接口"><a href="#智能体环境接口" class="headerlink" title="智能体环境接口"></a>智能体环境接口</h3><p>在强化学习中进行学习和决策的机器被称为是<strong>智能体</strong>，在智能体之外与其产生相互作用的的事物被称为是<strong>环境</strong>。智能体通过选择动作对环境产生一定的影响，环境对这些动作做出相应的响应。<br><a id="more"></a><br><a href="https://imgchr.com/i/B7CopF"><img data-src="https://s1.ax1x.com/2020/11/09/B7CopF.png" alt="B7CopF.png"></a><br>如上图所示：在每一个时间时刻智能体通过观察当前的环境$s_t\in  S$,选择对于当前环境来说最优的一个动作$a_t \in A$，在下一时刻，智能体从环境中收益$R_{t+1} \in R$,并且环境进入一个新的状态$s_{t+1} \in S$。</p>
<h3 id="目标和收益"><a href="#目标和收益" class="headerlink" title="目标和收益"></a>目标和收益</h3><p>在强化学习中智能体的目标被表示成从从环境获取的收益。在每一个离散的时刻智能体都会从环境中获取一个标量的值表示收益，而智能体的目标是获取累计的最大收益。我们需要通过设定合适的奖励函数来促使智能体尽快的实现目标，实现我们设计强化学习的目的。<br>例如：<br>1 在迷宫游戏中，智能体在成功逃脱迷宫前每一步的收益都是-1，来促使智能体尽快的逃离迷宫。如果在智能体没有逃离迷宫之前每一步都基于正的收益，智能体将一直在迷宫中游荡。<br>2 在下棋游戏中，只有当赢得比赛时才给予智能体一个正值的=收益。如果吃掉对方棋子时也能活得收益，之恩那个提就会以输掉比赛为代价去吃掉对方的棋子。</p>
<h3 id="回报和分幕"><a href="#回报和分幕" class="headerlink" title="回报和分幕"></a>回报和分幕</h3><p>智能体的目标是最大化累计收益，一般来说我们寻求的是最大化期望回报，记为$G_t$,在最简单的情况下回报是收益的总和即：<script type="math/tex">G_t=R_{t+1}+R_{t+2}+R_{t+3}+……+R_T</script>其中T为最终时刻。<br>在一些情况下智能体和环境之间的交互可一划分为一系列子序列，我们称每一个子序列为幕（episodes），比如一盘游戏的结束。这时我们的状态将会分为两种一种为非终结状态集$S$，令一种是包含终结与非终结状态的集合$S^+$。<br>在另外一些情况下智能体和环境之间的交互不能分成不同的幕而是持续不断的发生，比如一个连续过程的控制任务。为此我们引入一个<strong>折扣</strong>的概念，在公式中用$\gamma$ 来表示折扣率。</p>
<script type="math/tex; mode=display">G_t=R_{t+1}+\gamma R_{t+2}+\gamma ^2 R_{t+3}+……=\sum_{k=0}^{\infty}\gamma ^kR_{t+k+1}</script><p>折扣率决定了未来收益的现值，当$\gamma=0$时表示该智能体是相当短视的只考虑眼前的利益而不考虑长远利益。当$\gamma=1$时表示该智能体是有远见的，但不符合常理。比如有一个选择：现在给你100美元和10年后给你100美元，如果$\gamma =1$之恩那个提会认为这两个选择所获得收益是相等的，这显然不符合常理，所以$\gamma$通常是一个大于零小于1的数。</p>
<h4 id="Excise3-6"><a href="#Excise3-6" class="headerlink" title="Excise3.6"></a>Excise3.6</h4><script type="math/tex; mode=display">\sum_{s'\in s^+} \sum_{r\in R}p(s',r|s,a)=1</script><h3 id="分幕式和持续性任务的统一标识方法"><a href="#分幕式和持续性任务的统一标识方法" class="headerlink" title="分幕式和持续性任务的统一标识方法"></a>分幕式和持续性任务的统一标识方法</h3><p><a href="https://imgchr.com/i/BHJVxg"><img data-src="https://s1.ax1x.com/2020/11/09/BHJVxg.png" alt="BHJVxg.png"></a><br>在上一节中提到了分幕式和持续性任务的两种不同的表示方法，为此我们加入 了一种特殊的吸收态，其特点是交互和转移过程均在自身进行并且奖励值为0，这样就不会对总体的$G_t$产生影响。使用这种方法，我么就可以把分幕式任务和持续型任务的回报函数采用一种统一的表示方法。<script type="math/tex">G_t = \sum_{k=t+1}^T \gamma^{k-t-1}R_k</script></p>
<h3 id="策略和价值函数"><a href="#策略和价值函数" class="headerlink" title="策略和价值函数"></a>策略和价值函数</h3><p><em>价值函数</em>： 价值函数是状态（状态和动作的二元）有关的函数，用来评估当前智能体在给定状态（状态和动作）下有多好。<br><em>策略</em>：策略是从状态到每个动作的选择概率之间的映射。$\pi(a|s)$就是当$S_t=s$时$A_t=a$的概率。<br>我们把策略$\pi$下状态$s$的价值函数记为$v_{\pi}(s)$，即从状态$s$开始，智能体按照策略$\pi$进行决策所获得的回报的概率期望值。计算公式为：</p>
<script type="math/tex; mode=display">v_{\pi}(s)=\mathbb{E}_{\pi}[G_t|S_t=s]=\mathbb{E}_{\pi}[\sum_{k=0}^\infty\gamma R_{t+k+1}|S_t=s]</script><p>在强化学习中价值函数与后继状态价值之间还有一个递推特性：</p>
<script type="math/tex; mode=display">
\begin{aligned}
v_{\pi}(s) &= \mathbb{E}_{\pi}[G_t|S_t=s]\\&=\mathbb{E}_{\pi}[R_{t+1}+\gamma G_{t+1}|S_t=s]\\&=\sum_a\pi(a|s)\sum_{s'}\sum_rp(s',r|s,a)[r+\gamma\mathbb{
E}_{\pi}[G_{t+1}|s_{t+1}=s']]\\&=\sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)[r+\gamma v_{\pi}(s')]
\end{aligned}</script><p>类似的，把在策略$\pi$下在状态$s$时采取动作$a$的价值记为$q_{pi}(s|a)$，这是根据策略$\pi$在状态$s$开始执行动作$a$之后，所有可能的决策序列的期望回报：</p>
<script type="math/tex; mode=display">q_{\pi}(s|a) = \mathbb{E}_{\pi}[G_t|S_t=s,A_t=a]=\mathbb{E}_{\pi}[\sum_{k=0}^{\infty}\gamma^kR_{t+k+1}|S_t=s,A_t=a]</script><p>下面的图片可以帮助整个过程。<br><a href="https://imgchr.com/i/DSSkaF"><img data-src="https://s3.ax1x.com/2020/11/12/DSSkaF.png" alt="DSSkaF.png"></a></p>
<h3 id="最优策略和最优价值函数"><a href="#最优策略和最优价值函数" class="headerlink" title="最优策略和最优价值函数"></a>最优策略和最优价值函数</h3><p>解决一个强化学习问题就意味着要找出一个策略$\pi’$使其期望的回报$v_{\pi’}(s)$大于其它任何策略的期望回报，我们称其为最优策略。其状态价值函数称为最优状态价值函数</p>
<script type="math/tex; mode=display">v_*(s)={\underset {\pi}{\operatorname {max} }}v_{\pi}(s)</script><p>最优策略共享最优动作价值函数</p>
<script type="math/tex; mode=display">q_*(s,q)={\underset {\pi}{\operatorname {max} }}q_{\pi}(s,a)</script><p>因为$v_*$是策略的价值函数，因此必须满足贝尔曼方程，称为贝尔曼最优方程，这个方程说明了在最优策略下各个状态的 价值一定等于这个状态下最优动作的期望回报。</p>
<script type="math/tex; mode=display">
\begin{aligned}
v_{*}(s) &=\max _{a \in \mathcal{A}(s)} q_{\pi_{*}}(s, a) \\
&=\max _{a} \mathbb{E}_{\pi_{*}}\left[G_{t} \mid S_{t}=s, A_{t}=a\right] \\
&=\max _{a} \mathbb{E}_{\pi_{*}}\left[R_{t+1}+\gamma G_{t+1} \mid S_{t}=s, A_{t}=a\right] \\
&=\max _{a} \mathbb{E}\left[R_{t+1}+\gamma v_{*}\left(S_{t+1}\right) \mid S_{t}=s, A_{t}=a\right] \\
&=\max _{a} \sum_{s^{\prime}, r} p\left(s^{\prime}, r \mid s, a\right)\left[r+\gamma v_{*}\left(s^{\prime}\right)\right]
\end{aligned}</script><h3 id="最优性和近似算法"><a href="#最优性和近似算法" class="headerlink" title="最优性和近似算法"></a>最优性和近似算法</h3><p>虽然我们已经定义出了最优值函数和最优策略，而且理论上也可以直接计算出来。但是通常情况下我们没法得到这么多的计算资源。与此同时内存溢出也是一个很大的问题，因为很多问题的状态数量太多超过存储范围。对于这些情况我们就不能够使用直接存储每个状态的值函数而是必须使用一种更精简的参数型函数表示的方法。<br>强化学习的框架迫使我们进行近似求解。而且这个框架同时也很容易进行近似，比如对于很多小概率出现的状态，选择最优解和次优解区别不大。而且强化学习的在线学习的特性让其能够方便地对出现比较多的状态进行更多关注。这是一个强化学习区分其它近似求解MDP的关键特点。</p>
]]></content>
      <categories>
        <category>Reinforcement Learning</category>
      </categories>
      <tags>
        <tag>Reinforcement Learning</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>强化学习Sutton-Chapter4-动态规划</title>
    <url>/2020/11/20/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0Sutton-Chapter4-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/</url>
    <content><![CDATA[<p><strong>动态规划</strong>是在给定一个用马尔可夫决策过程（MDP）描述的完备环境模型的情况下，其可以计算最优的策略。<br><a id="more"></a></p>
<h3 id="策略评估"><a href="#策略评估" class="headerlink" title="策略评估"></a>策略评估</h3><p>回顾上一章的内容，可知状态价值函数的计算方法如下，求解价值函数我们称之为<strong>策略评估</strong>。</p>
<script type="math/tex; mode=display">
\begin{aligned}
v_{\pi}(s) & \doteq \mathbb{E}_{\pi}\left[G_{t} \mid S_{t}=s\right] \\
&=\mathbb{E}_{\pi}\left[R_{t+1}+\gamma G_{t+1} \mid S_{t}=s\right] \\
&=\mathbb{E}_{\pi}\left[R_{t+1}+\gamma v_{\pi}\left(S_{t+1}\right) \mid S_{t}=s\right] \\
&=\sum_{a} \pi(a \mid s) \sum_{s^{\prime}, r} p\left(s^{\prime}, r \mid s, a\right)\left[r+\gamma v_{z}\left(s^{\prime}\right)\right],
\end{aligned} \tag1</script><p>通过将上式看作是一个具有$|S|$个未知数和$|S|$个方程的线性方程组，可以直接将$v_{\pi}(s)$解出，这种方法称之为<strong>解析法</strong>，通常该方法的计算过程较为繁琐。<br>我们通常使用<strong>迭代法</strong>，迭代的初始值$v_0$可以任意选取，然后下一步的取值近似的使用$v_{\pi}$的贝尔曼方程进行更新，当$k\rightarrow  \infty$时$\{v_k\}$将会收敛到$v_{\pi}$。</p>
<script type="math/tex; mode=display">
\begin{aligned}
v_{k+1}(s) & \doteq \mathbb{E}_{\pi}\left[R_{t+1}+\gamma v_{k}\left(S_{t+1}\right) \mid S_{t}=s\right] \\
&=\sum_{a} \pi(a \mid s) \sum_{s^{\prime}, r} p\left(s^{\prime}, r \mid s, a\right)\left[r+\gamma v_{k}\left(s^{\prime}\right)\right]
\end{aligned} \tag2</script><h4 id="Example-网格游戏"><a href="#Example-网格游戏" class="headerlink" title="Example 网格游戏"></a>Example 网格游戏</h4><h3 id="策略改进"><a href="#策略改进" class="headerlink" title="策略改进"></a>策略改进</h3><p>策略评估的作用就是为了寻找更好的策略，在$s$状态时如果继续遵守当前策略$\pi (s)$此时的状态价值为$v_{\pi}(s)$，选择动作$a$后，继续执行原策略$\pi$,这时根据策略改进定理，我们通过比较$q_{\pi}(s,a)$和$v_{\pi}$的值就可以知道当前动作更优还是原策略更优。</p>
<script type="math/tex; mode=display">
\begin{aligned}
q_{\pi}(s, a) & \doteq \mathbb{E}\left[R_{t+1}+\gamma v_{x}\left(S_{t+1}\right) \mid S_{t}=s, A_{t}=a\right] \\
&=\sum_{x^{\prime}, r} p\left(s^{\prime}, r \mid s, a\right)\left[r+\gamma v_{\pi}\left(s^{\prime}\right)\right]
\end{aligned} \tag3</script><p><strong>策略改进定理(policy improvement theorem)</strong>：$π$和$π’$是两个确定的策略，如果对所有状态$s∈S$有$Qπ(s,π’(s))≥Vπ(s)$，那么策略$π’$必然比策略$π$更好，或者至少一样好。其中的不等式等价于$Vπ’(s)≥Vπ(s)$。<br>我们将上述选择最优策略的方法 推广到所有的状态和动作中得到：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\pi^{\prime}(s) & \doteq \arg \max q_{\pi}(s, a) \\
&=\arg \max \mathbb{E}\left[R_{t+1}+\gamma v_{\pi}\left(S_{t+1}\right) \mid S_{t}=s, A_{t}=a\right] \\
&=\arg \max _{a} \sum_{x^{\prime}, r} p\left(s^{\prime}, r \mid s, a\right)\left[r+\gamma v_{\pi}\left(s^{\prime}\right)\right]
\end{aligned} \tag4</script><h3 id="策略迭代"><a href="#策略迭代" class="headerlink" title="策略迭代"></a>策略迭代</h3><p>假设我们有一个策略$\pi$, 我们可以通过策略评估获取策略的$v_{\pi}(s)$然后根据策略改进定理获取更好的策略$\pi’$,之后再次使用策略评估计算$v_{\pi’}(s)$,再次根据策略改进定理求出下一个策略一直循环直到收敛。如下图所示：<br><a href="https://imgchr.com/i/DlirDg"><img data-src="https://s3.ax1x.com/2020/11/20/DlirDg.png" alt="DlirDg.png"></a><br>具体算法实现过程如下图所示：<br><a href="https://imgchr.com/i/DliO8x"><img data-src="https://s3.ax1x.com/2020/11/20/DliO8x.png" alt="DliO8x.png"></a></p>
<h3 id="价值迭代"><a href="#价值迭代" class="headerlink" title="价值迭代"></a>价值迭代</h3><p>策略迭代的方法有一个显著的缺点就是需要巨大的计算量，使用我的计算机在杰克租车问题中设置收敛阈值为0.0001，仅策略评估收敛计算的时间大约需要50mins。实际上我们可以通过截断策略评估的进程来减少计算量而对贪心策略没有任何影响（比如在方格游戏中前三轮评估之后的迭代对贪心策略没有任何影响）。提前截断的极端情况就是遍历一遍策略评估就立刻停止策略评估的进程进入策略改进，我们称中方法为<strong>值迭代</strong>。具体的实现算法如下图所示：<br> <a href="https://imgchr.com/i/DlAtUJ"><img data-src="https://s3.ax1x.com/2020/11/20/DlAtUJ.png" alt="DlAtUJ.png"></a></p>
<h3 id="异步动态规划"><a href="#异步动态规划" class="headerlink" title="异步动态规划"></a>异步动态规划</h3><p> 动态规划的一个主要缺点就是需要对MDP整个状态集进行遍历，如果状态集过大，仅一次遍历就需要十分浪费计算资源。<strong>异步动态规划</strong>是一种就地迭代的算法，这个算法使用任意可用的状态值，以任意顺序来更新状态值。我们也可以选择特定的需要更新的状态值进行多次更新和跳过一些特定装状态值的更新来节省资源提高系统的的灵活性。</p>
]]></content>
      <categories>
        <category>Reinforcement Learning</category>
      </categories>
      <tags>
        <tag>Reinforcement Learning</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
</search>
