<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hello World</title>
    <url>/2020/11/02/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.<br><a id="more"></a></p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>强化学习Sutton-Chapter3-有限马尔可夫决策过程</title>
    <url>/2020/11/13/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0Sutton-Chapter3-%E6%9C%89%E9%99%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/</url>
    <content><![CDATA[<h3 id="智能体环境接口"><a href="#智能体环境接口" class="headerlink" title="智能体环境接口"></a>智能体环境接口</h3><p>在强化学习中进行学习和决策的机器被称为是<strong>智能体</strong>，在智能体之外与其产生相互作用的的事物被称为是<strong>环境</strong>。智能体通过选择动作对环境产生一定的影响，环境对这些动作做出相应的响应。<br><a id="more"></a><br><a href="https://imgchr.com/i/B7CopF"><img data-src="https://s1.ax1x.com/2020/11/09/B7CopF.png" alt="B7CopF.png"></a><br>如上图所示：在每一个时间时刻智能体通过观察当前的环境$s_t\in  S$,选择对于当前环境来说最优的一个动作$a_t \in A$，在下一时刻，智能体从环境中收益$R_{t+1} \in R$,并且环境进入一个新的状态$s_{t+1} \in S$。</p>
<h3 id="目标和收益"><a href="#目标和收益" class="headerlink" title="目标和收益"></a>目标和收益</h3><p>在强化学习中智能体的目标被表示成从从环境获取的收益。在每一个离散的时刻智能体都会从环境中获取一个标量的值表示收益，而智能体的目标是获取累计的最大收益。我们需要通过设定合适的奖励函数来促使智能体尽快的实现目标，实现我们设计强化学习的目的。<br>例如：<br>1 在迷宫游戏中，智能体在成功逃脱迷宫前每一步的收益都是-1，来促使智能体尽快的逃离迷宫。如果在智能体没有逃离迷宫之前每一步都基于正的收益，智能体将一直在迷宫中游荡。<br>2 在下棋游戏中，只有当赢得比赛时才给予智能体一个正值的=收益。如果吃掉对方棋子时也能活得收益，之恩那个提就会以输掉比赛为代价去吃掉对方的棋子。</p>
<h3 id="回报和分幕"><a href="#回报和分幕" class="headerlink" title="回报和分幕"></a>回报和分幕</h3><p>智能体的目标是最大化累计收益，一般来说我们寻求的是最大化期望回报，记为$G_t$,在最简单的情况下回报是收益的总和即：<script type="math/tex">G_t=R_{t+1}+R_{t+2}+R_{t+3}+……+R_T</script>其中T为最终时刻。<br>在一些情况下智能体和环境之间的交互可一划分为一系列子序列，我们称每一个子序列为幕（episodes），比如一盘游戏的结束。这时我们的状态将会分为两种一种为非终结状态集$S$，令一种是包含终结与非终结状态的集合$S^+$。<br>在另外一些情况下智能体和环境之间的交互不能分成不同的幕而是持续不断的发生，比如一个连续过程的控制任务。为此我们引入一个<strong>折扣</strong>的概念，在公式中用$\gamma$ 来表示折扣率。</p>
<script type="math/tex; mode=display">G_t=R_{t+1}+\gamma R_{t+2}+\gamma ^2 R_{t+3}+……=\sum_{k=0}^{\infty}\gamma ^kR_{t+k+1}</script><p>折扣率决定了未来收益的现值，当$\gamma=0$时表示该智能体是相当短视的只考虑眼前的利益而不考虑长远利益。当$\gamma=1$时表示该智能体是有远见的，但不符合常理。比如有一个选择：现在给你100美元和10年后给你100美元，如果$\gamma =1$之恩那个提会认为这两个选择所获得收益是相等的，这显然不符合常理，所以$\gamma$通常是一个大于零小于1的数。</p>
<h4 id="Excise3-6"><a href="#Excise3-6" class="headerlink" title="Excise3.6"></a>Excise3.6</h4><script type="math/tex; mode=display">\sum_{s'\in s^+} \sum_{r\in R}p(s',r|s,a)=1</script><h3 id="分幕式和持续性任务的统一标识方法"><a href="#分幕式和持续性任务的统一标识方法" class="headerlink" title="分幕式和持续性任务的统一标识方法"></a>分幕式和持续性任务的统一标识方法</h3><p><a href="https://imgchr.com/i/BHJVxg"><img data-src="https://s1.ax1x.com/2020/11/09/BHJVxg.png" alt="BHJVxg.png"></a><br>在上一节中提到了分幕式和持续性任务的两种不同的表示方法，为此我们加入 了一种特殊的吸收态，其特点是交互和转移过程均在自身进行并且奖励值为0，这样就不会对总体的$G_t$产生影响。使用这种方法，我么就可以把分幕式任务和持续型任务的回报函数采用一种统一的表示方法。<script type="math/tex">G_t = \sum_{k=t+1}^T \gamma^{k-t-1}R_k</script></p>
<h3 id="策略和价值函数"><a href="#策略和价值函数" class="headerlink" title="策略和价值函数"></a>策略和价值函数</h3><p><em>价值函数</em>： 价值函数是状态（状态和动作的二元）有关的函数，用来评估当前智能体在给定状态（状态和动作）下有多好。<br><em>策略</em>：策略是从状态到每个动作的选择概率之间的映射。$\pi(a|s)$就是当$S_t=s$时$A_t=a$的概率。<br>我们把策略$\pi$下状态$s$的价值函数记为$v_{\pi}(s)$，即从状态$s$开始，智能体按照策略$\pi$进行决策所获得的回报的概率期望值。计算公式为：</p>
<script type="math/tex; mode=display">v_{\pi}(s)=\mathbb{E}_{\pi}[G_t|S_t=s]=\mathbb{E}_{\pi}[\sum_{k=0}^\infty\gamma R_{t+k+1}|S_t=s]</script><p>在强化学习中价值函数与后继状态价值之间还有一个递推特性：</p>
<script type="math/tex; mode=display">
\begin{aligned}
v_{\pi}(s) &= \mathbb{E}_{\pi}[G_t|S_t=s]\\&=\mathbb{E}_{\pi}[R_{t+1}+\gamma G_{t+1}|S_t=s]\\&=\sum_a\pi(a|s)\sum_{s'}\sum_rp(s',r|s,a)[r+\gamma\mathbb{
E}_{\pi}[G_{t+1}|s_{t+1}=s']]\\&=\sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)[r+\gamma v_{\pi}(s')]
\end{aligned}</script><p>类似的，把在策略$\pi$下在状态$s$时采取动作$a$的价值记为$q_{pi}(s|a)$，这是根据策略$\pi$在状态$s$开始执行动作$a$之后，所有可能的决策序列的期望回报：</p>
<script type="math/tex; mode=display">q_{\pi}(s|a) = \mathbb{E}_{\pi}[G_t|S_t=s,A_t=a]=\mathbb{E}_{\pi}[\sum_{k=0}^{\infty}\gamma^kR_{t+k+1}|S_t=s,A_t=a]</script><p>下面的图片可以帮助整个过程。<br><a href="https://imgchr.com/i/DSSkaF"><img data-src="https://s3.ax1x.com/2020/11/12/DSSkaF.png" alt="DSSkaF.png"></a></p>
<h3 id="最优策略和最优价值函数"><a href="#最优策略和最优价值函数" class="headerlink" title="最优策略和最优价值函数"></a>最优策略和最优价值函数</h3><p>解决一个强化学习问题就意味着要找出一个策略$\pi’$使其期望的回报$v_{\pi’}(s)$大于其它任何策略的期望回报，我们称其为最优策略。其状态价值函数称为最优状态价值函数</p>
<script type="math/tex; mode=display">v_*(s)={\underset {\pi}{\operatorname {max} }}v_{\pi}(s)</script><p>最优策略共享最优动作价值函数</p>
<script type="math/tex; mode=display">q_*(s,q)={\underset {\pi}{\operatorname {max} }}q_{\pi}(s,a)</script><p>因为$v_*$是策略的价值函数，因此必须满足贝尔曼方程，称为贝尔曼最优方程，这个方程说明了在最优策略下各个状态的 价值一定等于这个状态下最优动作的期望回报。</p>
<script type="math/tex; mode=display">
\begin{aligned}
v_{*}(s) &=\max _{a \in \mathcal{A}(s)} q_{\pi_{*}}(s, a) \\
&=\max _{a} \mathbb{E}_{\pi_{*}}\left[G_{t} \mid S_{t}=s, A_{t}=a\right] \\
&=\max _{a} \mathbb{E}_{\pi_{*}}\left[R_{t+1}+\gamma G_{t+1} \mid S_{t}=s, A_{t}=a\right] \\
&=\max _{a} \mathbb{E}\left[R_{t+1}+\gamma v_{*}\left(S_{t+1}\right) \mid S_{t}=s, A_{t}=a\right] \\
&=\max _{a} \sum_{s^{\prime}, r} p\left(s^{\prime}, r \mid s, a\right)\left[r+\gamma v_{*}\left(s^{\prime}\right)\right]
\end{aligned}</script><h3 id="最优性和近似算法"><a href="#最优性和近似算法" class="headerlink" title="最优性和近似算法"></a>最优性和近似算法</h3><p>虽然我们已经定义出了最优值函数和最优策略，而且理论上也可以直接计算出来。但是通常情况下我们没法得到这么多的计算资源。与此同时内存溢出也是一个很大的问题，因为很多问题的状态数量太多超过存储范围。对于这些情况我们就不能够使用直接存储每个状态的值函数而是必须使用一种更精简的参数型函数表示的方法。<br>强化学习的框架迫使我们进行近似求解。而且这个框架同时也很容易进行近似，比如对于很多小概率出现的状态，选择最优解和次优解区别不大。而且强化学习的在线学习的特性让其能够方便地对出现比较多的状态进行更多关注。这是一个强化学习区分其它近似求解MDP的关键特点。</p>
]]></content>
      <categories>
        <category>Reinforcement Learning</category>
      </categories>
      <tags>
        <tag>Reinforcement Learning</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
</search>
